# -*- coding: utf-8 -*-
"""analyzing_hardest_char_for_none_touch_typists.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dvgj6wndwZ4bf6Zw11pTZYxGii19IzEG
"""

!pip install opencv-python

!gdown 1rkgV4RneyTiL_5-vDlMi5LoDEOo_iYHz
!gdown 1TdzDcD0FeyUvu7JNulANCS9v9zOPPCLQ

import cv2
import os

video_path = "/content/1491577862552_10_-study-benefits_of_running_writing.webm"
output_folder = "frames"

os.makedirs(output_folder, exist_ok=True)

cap = cv2.VideoCapture(video_path)
frame_number = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame_filename = os.path.join(output_folder, f"frame_{frame_number:05d}.png")
    cv2.imwrite(frame_filename, frame)

    frame_number += 1

cap.release()
print(f"Extracted {frame_number} frames.")

!pip install mediapipe

import mediapipe as mp
import pandas as pd

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)

import glob

frames_folder = "frames_output"
output_csv = "landmarks_output.csv"

data_rows = []

for frame_file in sorted(glob.glob(f"{frames_folder}/*.png")):
    frame_number = int(os.path.basename(frame_file).split('_')[1].split('.')[0])
    image = cv2.imread(frame_file)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = face_mesh.process(image_rgb)

    if results.multi_face_landmarks:
        for i, landmark in enumerate(results.multi_face_landmarks[0].landmark):
            data_rows.append({
                "frame": frame_number,
                "landmark_id": i,
                "x": landmark.x,
                "y": landmark.y,
                "z": landmark.z
            })

df = pd.DataFrame(data_rows)
df.to_csv(output_csv, index=False)
print(f" Saved landmarks to {output_csv}")

import cv2
import os

video_path = '/content/1491577862552_10_-study-benefits_of_running_writing.webm'
output_folder = '/content/frames_output'

os.makedirs(output_folder, exist_ok=True)

cap = cv2.VideoCapture(video_path)

frame_count = 0
success, frame = cap.read()

while success:
    frame_filename = os.path.join(output_folder, f"frame_{frame_count:05d}.png")
    cv2.imwrite(frame_filename, frame)
    frame_count += 1
    success, frame = cap.read()

cap.release()
print(f" Done! Extracted {frame_count} frames to {output_folder}")

frame_path = '/content/frames_output/frame_00010.png'

import mediapipe as mp
import cv2
import matplotlib.pyplot as plt

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)

#  Load the extracted image frame
image = cv2.imread(frame_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

#  Run face mesh
results = face_mesh.process(image_rgb)

if results.multi_face_landmarks:
    annotated_image = image.copy()
    for landmark in results.multi_face_landmarks[0].landmark:
        h, w, _ = image.shape
        cx, cy = int(landmark.x * w), int(landmark.y * h)
        cv2.circle(annotated_image, (cx, cy), 1, (0,255,0), -1)

    plt.figure(figsize=(10,10))
    plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()
else:
    print("❗️ No face detected in this frame")

import mediapipe as mp
import pandas as pd
import cv2
import glob
import os

#  Setup
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)

frames_folder = '/content/frames_output'    # Change if needed
output_csv = '/content/landmarks_all_frames.csv'

#  Storage for all landmark rows
all_rows = []

#  Process all frames
frame_files = sorted(glob.glob(os.path.join(frames_folder, '*.png')))
print(f" Found {len(frame_files)} frames to process.")

for frame_file in frame_files:
    frame_number = int(os.path.basename(frame_file).split('_')[1].split('.')[0])
    image = cv2.imread(frame_file)
    if image is None:
        print(f"⚠ Could not read {frame_file}")
        continue

    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = face_mesh.process(image_rgb)

    if results.multi_face_landmarks:
        for i, landmark in enumerate(results.multi_face_landmarks[0].landmark):
            all_rows.append({
                "frame": frame_number,
                "landmark_id": i,
                "x": landmark.x,
                "y": landmark.y,
                "z": landmark.z
            })

#  Save all to CSV
df = pd.DataFrame(all_rows)
df.to_csv(output_csv, index=False)
print(f" Saved all landmarks to {output_csv}")

import cv2
import numpy as np
import matplotlib.pyplot as plt
import mediapipe as mp

#  Mediapipe setup
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)

#  Load ONE frame
frame_path = '/content/frames_output/frame_00010.png'
image = cv2.imread(frame_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
h, w, _ = image.shape

#  Process with FaceMesh
results = face_mesh.process(image_rgb)

if not results.multi_face_landmarks:
    print(" No face detected in this frame")
else:
    landmarks = results.multi_face_landmarks[0].landmark

    #  Define landmark IDs for pose estimation
    LANDMARK_MAP = {
        "nose_tip": 1,
        "chin": 152,
        "left_eye": 263,
        "right_eye": 33,
        "left_mouth": 287,
        "right_mouth": 57
    }

    #  Collect 2D image points
    image_points = []
    for name, lid in LANDMARK_MAP.items():
        lm = landmarks[lid]
        x = lm.x * w
        y = lm.y * h
        image_points.append([x, y])

    image_points = np.array(image_points, dtype=np.float64)
    print(" 2D image points:\n", image_points)

    #  Plot them on the face
    annotated = image.copy()
    for (x, y) in image_points:
        cv2.circle(annotated, (int(x), int(y)), 5, (0, 255, 0), -1)

    plt.figure(figsize=(8,8))
    plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.title('Pose landmarks on face')
    plt.show()

    #  Define 3D model points (approximate)
    MODEL_POINTS = np.array([
        [0.0, 0.0, 0.0],       # Nose tip
        [0.0, -63.6, -12.5],   # Chin
        [-43.3, 32.7, -26.0],  # Left eye corner
        [43.3, 32.7, -26.0],   # Right eye corner
        [-28.9, -28.9, -24.1], # Left mouth
        [28.9, -28.9, -24.1],  # Right mouth
    ])

    #  Camera intrinsics (approximate)
    focal_length = w
    center = (w/2, h/2)
    camera_matrix = np.array([
        [focal_length, 0, center[0]],
        [0, focal_length, center[1]],
        [0, 0, 1]
    ], dtype="double")

    dist_coeffs = np.zeros((4,1))

    #  Solve PnP
    success, rotation_vector, translation_vector = cv2.solvePnP(
        MODEL_POINTS,
        image_points,
        camera_matrix,
        dist_coeffs,
        flags=cv2.SOLVEPNP_ITERATIVE
    )

    if not success:
        print(" PnP failed.")
    else:
        print(" Rotation Vector:\n", rotation_vector)
        print(" Translation Vector:\n", translation_vector)

        #  Convert rotation vector to rotation matrix
        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)

        #  Get Euler angles (pitch, yaw, roll)
        sy = np.sqrt(rotation_matrix[0,0]**2 + rotation_matrix[1,0]**2)
        singular = sy < 1e-6

        if not singular:
            x = np.arctan2(rotation_matrix[2,1], rotation_matrix[2,2])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = np.arctan2(rotation_matrix[1,0], rotation_matrix[0,0])
        else:
            x = np.arctan2(-rotation_matrix[1,2], rotation_matrix[1,1])
            y = np.arctan2(-rotation_matrix[2,0], sy)
            z = 0

        pitch = np.degrees(x)
        yaw = np.degrees(y)
        roll = np.degrees(z)

        print(f"\n Estimated angles:")
        print(f"Pitch (up/down): {pitch:.2f}°")
        print(f"Yaw (left/right): {yaw:.2f}°")
        print(f"Roll (tilt): {roll:.2f}°")

import numpy as np
import pandas as pd
import cv2

#  Load landmark CSV
df = pd.read_csv('/content/landmarks_all_frames.csv')

#  Define landmark IDs for pose estimation
LANDMARK_MAP = {
    "nose_tip": 1,
    "chin": 152,
    "left_eye": 263,
    "right_eye": 33,
    "left_mouth": 287,
    "right_mouth": 57
}

#  3D model points in *some canonical space* (these are approximations!)
MODEL_POINTS = np.array([
    [0.0, 0.0, 0.0],       # Nose tip
    [0.0, -63.6, -12.5],   # Chin
    [-43.3, 32.7, -26.0],  # Left eye corner
    [43.3, 32.7, -26.0],   # Right eye corner
    [-28.9, -28.9, -24.1], # Left mouth
    [28.9, -28.9, -24.1],  # Right mouth
])

#  Results storage
pose_results = []

for frame_num in sorted(df['frame'].unique()):
    frame_data = df[df['frame'] == frame_num]

    # Get 2D image points for these landmarks
    image_points = []
    for name, lid in LANDMARK_MAP.items():
        landmark_row = frame_data[frame_data['landmark_id'] == lid]
        if landmark_row.empty:
            break
        x = landmark_row['x'].values[0]
        y = landmark_row['y'].values[0]
        image_points.append([x, y])

    if len(image_points) != 6:
        continue

    image_points = np.array(image_points, dtype=np.float64)

    # Scale normalized points to pixel coordinates
    # Assume webcam image size ~640x480 (adjust if yours is different)
    image_points[:, 0] *= 640
    image_points[:, 1] *= 480

    # Camera matrix approximation
    focal_length = 640
    center = (320, 240)
    camera_matrix = np.array(
        [[focal_length, 0, center[0]],
         [0, focal_length, center[1]],
         [0, 0, 1]], dtype = "double"
    )

    dist_coeffs = np.zeros((4,1))  # Assume no lens distortion

    # SolvePnP
    success, rotation_vector, translation_vector = cv2.solvePnP(
        MODEL_POINTS, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE)

    if not success:
        continue

    # Convert rotation vector to rotation matrix
    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)

    # Get Euler angles
    sy = np.sqrt(rotation_matrix[0,0]**2 + rotation_matrix[1,0]**2)
    singular = sy < 1e-6

    if not singular:
        x = np.arctan2(rotation_matrix[2,1], rotation_matrix[2,2])
        y = np.arctan2(-rotation_matrix[2,0], sy)
        z = np.arctan2(rotation_matrix[1,0], rotation_matrix[0,0])
    else:
        x = np.arctan2(-rotation_matrix[1,2], rotation_matrix[1,1])
        y = np.arctan2(-rotation_matrix[2,0], sy)
        z = 0

    # Convert radians to degrees
    pitch = np.degrees(x)
    yaw = np.degrees(y)
    roll = np.degrees(z)

    pose_results.append({
        "frame": frame_num,
        "pitch": pitch,
        "yaw": yaw,
        "roll": roll
    })

#  Save head pose angles
pose_df = pd.DataFrame(pose_results)
pose_df.to_csv('/content/head_pose_angles.csv', index=False)
print(f" Saved head pose angles to /content/head_pose_angles.csv")

import pandas as pd
import matplotlib.pyplot as plt

#  Load head pose data
pose_df = pd.read_csv('/content/head_pose_angles.csv')

#  Print min and max pitch
min_pitch = pose_df['pitch'].min()
max_pitch = pose_df['pitch'].max()

print(f" Minimum pitch: {min_pitch:.2f}°")
print(f" Maximum pitch: {max_pitch:.2f}°")

#  Plot pitch over time
plt.figure(figsize=(12, 6))
plt.plot(pose_df['frame'], pose_df['pitch'], label='Pitch (up/down)', color='orange')
plt.axhline(-20, color='red', linestyle='--', label='Keyboard threshold (-20°)')
plt.xlabel('Frame')
plt.ylabel('Pitch Angle (degrees)')
plt.title('Pitch Angle Over Time')
plt.legend()
plt.grid()
plt.show()

#  Histogram to see distribution
plt.figure(figsize=(8, 5))
plt.hist(pose_df['pitch'], bins=50, color='skyblue', edgecolor='black')
plt.axvline(-20, color='red', linestyle='--', label='Keyboard threshold (-20°)')
plt.title('Distribution of Pitch Angles')
plt.xlabel('Pitch (degrees)')
plt.ylabel('Count')
plt.legend()
plt.grid()
plt.show()

import pandas as pd

#  Load your head pose CSV
pose_df = pd.read_csv('/content/head_pose_angles.csv')

#  Adjusted threshold for MacBook Pro usage
pose_df['label'] = pose_df['pitch'].apply(lambda x: 'keyboard' if x < -10 else 'screen')

#  Save labeled CSV
pose_df.to_csv('/content/head_pose_labeled.csv', index=False)
print(" Saved labeled head pose to /content/head_pose_labeled.csv")

#  Print label counts
print("\n Label distribution:")
print(pose_df['label'].value_counts())



"""##second part

"""

!gdown 1MoyFWFh5ASit03HwR5TCgzfTdreex79h

import json

# File path
input_file = "/content/1491577862552.json"

# Target session
target_session = "1491577862552_10_/study/benefits_of_running_writing"

# Store extracted typing events
typing_events = []

# Load the whole JSON array at once
with open(input_file, "r", encoding="utf-8") as f:
    data = json.load(f)

# Filter for textInput in the specific session
for entry in data:
    if (
        entry.get("type") == "textInput" and
        "benefits_of_running_writing" in entry.get("sessionId", "")
    ):
        typing_events.append({
            "char": entry["text"][-1] if entry["text"] else "",
            "full_text": entry["text"],
            "x": entry["pos"]["left"],
            "y": entry["pos"]["top"],
            "timestamp": entry["epoch"]
        })

# Output results
for e in typing_events:
    print(e)

!gdown 1jpfDS27UB4tvWY4gCJagboVCjesDBqa6

import json
import pandas as pd

# Load typing timestamps and get range
typing_df = pd.DataFrame(typing_events)
typing_df['timestamp'] = pd.to_datetime(typing_df['timestamp'], unit='ms')

start_time = int(typing_df['timestamp'].min().timestamp() * 1000) - 200  # add margin
end_time = int(typing_df['timestamp'].max().timestamp() * 1000) + 200

# Load and filter gaze data
gaze_file = "/content/P_07.txt"
gaze_matches = []

with open(gaze_file, "r", encoding="utf-8") as f:
    for line in f:
        try:
            entry = json.loads(line)
            ts_ms = int(entry.get("true_time", 0) * 1000)

            # Basic validation
            x, y = entry["right_gaze_point_on_display_area"]
            if not (0 <= x <= 1) or not (0 <= y <= 1):
                continue  # skip invalid points

            if start_time <= ts_ms <= end_time:
                gaze_matches.append({
                    "timestamp": ts_ms,
                    "x_norm": x,
                    "y_norm": y
                })
        except (json.JSONDecodeError, KeyError, TypeError):
            continue

# Optional sanity check
print(f"Extracted {len(gaze_matches)} gaze points.")
print("Example:", gaze_matches[:3])

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#Config
screen_width = 1440
screen_height = 900

#Typing Data
typing_df = pd.DataFrame(typing_events)
typing_df['timestamp'] = pd.to_datetime(typing_df['timestamp'], unit='ms')

#Gaze Data
gaze_df = pd.DataFrame(gaze_matches)
gaze_df['x_px'] = gaze_df['x_norm'] * screen_width
gaze_df['y_px'] = gaze_df['y_norm'] * screen_height
gaze_df['timestamp'] = pd.to_datetime(gaze_df['timestamp'], unit='ms')

#Interpolation
gaze_df.set_index('timestamp', inplace=True)
gaze_df = gaze_df[~gaze_df.index.duplicated(keep='first')]
interp_gaze = gaze_df[['x_px', 'y_px']].sort_index().interpolate(method='time')

#Align gaze with typing timestamps
aligned_gaze = interp_gaze.reindex(typing_df['timestamp'], method='nearest', tolerance=pd.Timedelta(milliseconds=50))
typing_df['gaze_x_px'] = aligned_gaze['x_px'].values
typing_df['gaze_y_px'] = aligned_gaze['y_px'].values

#Compute distance
typing_df['gaze_distance'] = np.sqrt(
    (typing_df['x'] - typing_df['gaze_x_px'])**2 +
    (typing_df['y'] - typing_df['gaze_y_px'])**2
)

#Correlation
x_corr = typing_df['x'].corr(typing_df['gaze_x_px'])
y_corr = typing_df['y'].corr(typing_df['gaze_y_px'])
print("Correlation after interpolation (X):", x_corr)
print("Correlation after interpolation (Y):", y_corr)

#Plot X
plt.figure(figsize=(9, 5))
plt.plot(typing_df['timestamp'], typing_df['x'], label='Typing X', color='orange')
plt.plot(typing_df['timestamp'], typing_df['gaze_x_px'], label='Interpolated Gaze X', color='blue')
plt.xlabel("Time")
plt.ylabel("X Position (px)")
plt.title("Typing vs Gaze X over Time")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#Plot Y
plt.figure(figsize=(9, 5))
plt.plot(typing_df['timestamp'], typing_df['y'], label='Typing Y', color='orange')
plt.plot(typing_df['timestamp'], typing_df['gaze_y_px'], label='Interpolated Gaze Y', color='blue')
plt.xlabel("Time")
plt.ylabel("Y Position (px)")
plt.title("Typing vs Gaze Y over Time")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

def detect_fixations(gaze_df, std_threshold=35, window_duration=300):
    """
    Detect fixations in gaze data using a sliding window approach.

    Parameters:
    - gaze_df: DataFrame with ['timestamp', 'x_px', 'y_px'] in pixel space.
    - std_threshold: Max std deviation (in px) to be considered a fixation.
    - window_duration: Sliding window in milliseconds.

    Returns:
    - fixations_df: DataFrame with ['start_time', 'end_time', 'duration', 'mean_x', 'mean_y']
    """
    fixations = []
    gaze_df = gaze_df.reset_index().sort_values("timestamp")  # ensure timestamp is a column

    start_idx = 0
    while start_idx < len(gaze_df):
        start_time = gaze_df.loc[start_idx, 'timestamp']
        end_time = start_time + pd.Timedelta(milliseconds=window_duration)
        window = gaze_df[(gaze_df['timestamp'] >= start_time) & (gaze_df['timestamp'] <= end_time)]

        if len(window) < 5:
            start_idx += 1
            continue

        std_x = window['x_px'].std()
        std_y = window['y_px'].std()

        if std_x < std_threshold and std_y < std_threshold:
            fixation = {
                'start_time': window.iloc[0]['timestamp'],
                'end_time': window.iloc[-1]['timestamp'],
                'duration': (window.iloc[-1]['timestamp'] - window.iloc[0]['timestamp']).total_seconds() * 1000,
                'mean_x': window['x_px'].mean(),
                'mean_y': window['y_px'].mean()
            }
            fixations.append(fixation)

            # Move index to first point after fixation
            start_idx = gaze_df[gaze_df['timestamp'] > fixation['end_time']].index.min()
            if pd.isna(start_idx):
                break
        else:
            start_idx += 1

    return pd.DataFrame(fixations)

fixations_df = detect_fixations(gaze_df)
print("Detected", len(fixations_df), "fixations")
fixations_df.head()

def align_fixations_to_typing(typing_df, fixations_df):
    """
    For each typing event, find the most recent fixation before the keypress.

    Adds 3 columns to typing_df:
    - fixation_end_time
    - fixation_lag_ms
    - fixation_distance

    Returns:
    - updated typing_df with fixation info
    """
    fixation_end_times = []
    fixation_lags = []
    fixation_distances = []

    for _, row in typing_df.iterrows():
        keypress_time = row['timestamp']

        # Find the last fixation ending before the keypress
        previous_fixations = fixations_df[fixations_df['end_time'] < keypress_time]

        if not previous_fixations.empty:
            last_fix = previous_fixations.iloc[-1]

            fixation_end_times.append(last_fix['end_time'])
            lag_ms = (keypress_time - last_fix['end_time']).total_seconds() * 1000
            fixation_lags.append(lag_ms)

            # Compute spatial distance from fixation to key
            dist = np.sqrt((row['x'] - last_fix['mean_x'])**2 + (row['y'] - last_fix['mean_y'])**2)
            fixation_distances.append(dist)
        else:
            fixation_end_times.append(None)
            fixation_lags.append(None)
            fixation_distances.append(None)

    typing_df['fixation_end_time'] = fixation_end_times
    typing_df['fixation_lag_ms'] = fixation_lags
    typing_df['fixation_distance'] = fixation_distances

    return typing_df

typing_df = align_fixations_to_typing(typing_df, fixations_df)

# Check output
typing_df[['char', 'timestamp', 'fixation_end_time', 'fixation_lag_ms', 'fixation_distance']].head()

import matplotlib.pyplot as plt

# Drop NaNs
valid_fix = typing_df.dropna(subset=['fixation_lag_ms', 'fixation_distance'])

# Histogram of fixation lag
plt.figure(figsize=(8, 4))
plt.hist(valid_fix['fixation_lag_ms'], bins=30, color='skyblue', edgecolor='black')
plt.title("Time Between Fixation End and Keypress")
plt.xlabel("Fixation Lag (ms)")
plt.ylabel("Count")
plt.grid(True)
plt.tight_layout()
plt.show()

# Histogram of spatial offset
plt.figure(figsize=(8, 4))
plt.hist(valid_fix['fixation_distance'], bins=30, color='salmon', edgecolor='black')
plt.title("Distance Between Fixation and Key")
plt.xlabel("Distance (pixels)")
plt.ylabel("Count")
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Ensure fixations and typing timestamps are datetime
fixations_df = fixations_df.sort_values("end_time")
typing_df = typing_df.sort_values("timestamp")

# Plot X over time
plt.figure(figsize=(9, 5))
plt.plot(typing_df['timestamp'], typing_df['x'], label='Typed X', color='orange')
plt.plot(typing_df['timestamp'], typing_df['fixation_end_time'].map(
    dict(zip(fixations_df['end_time'], fixations_df['mean_x']))
), label='Fixation X', color='red')
plt.xlabel("Time")
plt.ylabel("X Position (px)")
plt.title("Fixation X vs Typed X over Time")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot Y over time
plt.figure(figsize=(9, 5))
plt.plot(typing_df['timestamp'], typing_df['y'], label='Typed Y', color='orange')
plt.plot(typing_df['timestamp'], typing_df['fixation_end_time'].map(
    dict(zip(fixations_df['end_time'], fixations_df['mean_y']))
), label='Fixation Y', color='red')
plt.xlabel("Time")
plt.ylabel("Y Position (px)")
plt.title("Fixation Y vs Typed Y over Time")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Drop rows without matched fixations
fixation_char_pairs = typing_df.dropna(subset=['fixation_end_time'])

# Format result
fixation_char_list = fixation_char_pairs[[
    'char',
    'timestamp',
    'fixation_end_time',
    'fixation_lag_ms',
    'fixation_distance',
]]

# Add optional fixation details
fixation_char_list['fixation_start_time'] = fixation_char_list['fixation_end_time'].map(
    dict(zip(fixations_df['end_time'], fixations_df['start_time']))
)
fixation_char_list['fixation_mean_x'] = fixation_char_list['fixation_end_time'].map(
    dict(zip(fixations_df['end_time'], fixations_df['mean_x']))
)
fixation_char_list['fixation_mean_y'] = fixation_char_list['fixation_end_time'].map(
    dict(zip(fixations_df['end_time'], fixations_df['mean_y']))
)

# Reorder columns
fixation_char_list = fixation_char_list[[
    'char', 'timestamp', 'fixation_start_time', 'fixation_end_time',
    'fixation_lag_ms', 'fixation_distance', 'fixation_mean_x', 'fixation_mean_y'
]]

# View first rows
fixation_char_list.head(10)

from scipy.stats import pearsonr

# Step 1: Align fixation X and Y to typing timestamps
fixation_x_aligned = typing_df['fixation_end_time'].map(
    dict(zip(fixations_df['end_time'], fixations_df['mean_x']))
)
fixation_y_aligned = typing_df['fixation_end_time'].map(
    dict(zip(fixations_df['end_time'], fixations_df['mean_y']))
)

# Step 2: Drop any rows with missing values
valid_idx = fixation_x_aligned.notna() & fixation_y_aligned.notna()

# Step 3: Compute Pearson correlation
corr_x, _ = pearsonr(typing_df.loc[valid_idx, 'x'], fixation_x_aligned[valid_idx])
corr_y, _ = pearsonr(typing_df.loc[valid_idx, 'y'], fixation_y_aligned[valid_idx])

print(" Correlation between fixation X and typed X:", round(corr_x, 3))
print(" Correlation between fixation Y and typed Y:", round(corr_y, 3))

typing_df = pd.DataFrame(typing_events)
typing_df['timestamp'] = pd.to_datetime(typing_df['timestamp'], unit='ms')

import pandas as pd
import numpy as np

# ------------------------------------------
# 1 Load head pose data with pitch/yaw/roll
# ------------------------------------------
pose_df = pd.read_csv('/content/head_pose_labeled.csv')

#  Add gaze_direction from yaw
def classify_yaw(yaw):
    if yaw < -10:
        return 'left'
    elif yaw > 10:
        return 'right'
    else:
        return 'center'

pose_df['gaze_direction'] = pose_df['yaw'].apply(classify_yaw)

#  Save updated labeled pose with gaze_direction
pose_df.to_csv('/content/head_pose_with_gaze_direction.csv', index=False)
print(" Saved updated head pose with gaze_direction column.")

# ------------------------------------------
# 2 Assign synthetic timestamps to head pose frames
# ------------------------------------------
# Estimate 30fps (~33.33ms per frame)
fps = 30
ms_per_frame = 1000 / fps

# Define video start time from typing events
video_start_time = typing_df['timestamp'].min()
pose_df['frame_time'] = video_start_time + pd.to_timedelta(pose_df['frame'] * ms_per_frame, unit='ms')

# ------------------------------------------
# 3 Align head pose data to typing event timestamps
# ------------------------------------------
pose_df = pose_df.set_index('frame_time').sort_index()
typing_df = typing_df.set_index('timestamp').sort_index()

#  Nearest join with ±100ms tolerance
aligned_pose = pose_df.reindex(typing_df.index, method='nearest', tolerance=pd.Timedelta(milliseconds=100))

#  Merge pose features into typing log
typing_with_pose = typing_df.copy()
typing_with_pose = typing_with_pose.join(
    aligned_pose[['frame', 'pitch', 'yaw', 'roll', 'label', 'gaze_direction']]
)

#  Reset index to restore 'timestamp' column
typing_with_pose = typing_with_pose.reset_index()

#  Drop rows without head pose match
typing_with_pose = typing_with_pose.dropna(subset=['pitch', 'yaw'])

# ------------------------------------------
# 4 Save merged result
# ------------------------------------------
typing_with_pose.to_csv('/content/typing_with_headpose_alignment.csv', index=False)
print(" Saved merged typing+headpose file to /content/typing_with_headpose_alignment.csv")

# ------------------------------------------
# 5 Show 5 samples for each gaze_direction
# ------------------------------------------
for direction in ['left', 'center', 'right']:
    print(f"\n 5 samples for gaze_direction = {direction}:")
    subset = typing_with_pose[typing_with_pose['gaze_direction'] == direction]
    if subset.empty:
        print(" No data for this category!")
    else:
        print(subset[['frame', 'timestamp', 'char', 'pitch', 'yaw', 'label', 'gaze_direction']].head(5))

import matplotlib.pyplot as plt
import cv2
import os

#  Filter 5 CENTER samples
center_samples = typing_with_pose[typing_with_pose['gaze_direction'] == 'center'].head(5)

#  Define your frames folder
frames_folder = '/content/frames_output'

#  Plot each frame image
for idx, row in center_samples.iterrows():
    frame_num = int(row['frame'])
    frame_filename = os.path.join(frames_folder, f"frame_{frame_num:05d}.png")

    # Load and check
    image = cv2.imread(frame_filename)
    if image is None:
        print(f" Could not load {frame_filename}")
        continue

    # Convert BGR to RGB for plotting
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Plot
    plt.figure(figsize=(6, 6))
    plt.imshow(image_rgb)
    plt.title(f"Frame {frame_num} | Char: {row['char']} | Pitch: {row['pitch']:.2f} | Yaw: {row['yaw']:.2f}")
    plt.axis('off')
    plt.show()

import pandas as pd
import numpy as np
import os

#  Load landmarks data
landmarks_csv = '/content/landmarks_all_frames.csv'
df = pd.read_csv(landmarks_csv)

print(" Loaded landmarks:", df.shape)

#  Define landmark IDs for eyes
LEFT_EYE_CORNERS = [33, 133]
RIGHT_EYE_CORNERS = [263, 362]

# You can also use MediaPipe iris center if available (~468+),
# but we'll use approximate eye centers:
LEFT_EYE_CENTER_IDS = [159, 145]
RIGHT_EYE_CENTER_IDS = [386, 374]


#  Process all frames
eye_results = []

for frame_num in df['frame'].unique():
    frame_data = df[df['frame'] == frame_num]

    # LEFT eye
    left_points = frame_data[frame_data['landmark_id'].isin(LEFT_EYE_CORNERS)]
    left_iris = frame_data[frame_data['landmark_id'].isin(LEFT_EYE_CENTER_IDS)]

    if len(left_points) < 2 or len(left_iris) < 1:
        continue

    lx0 = left_points.iloc[0]['x']
    lx1 = left_points.iloc[1]['x']
    left_width = abs(lx1 - lx0)
    left_center_x = (lx0 + lx1) / 2
    left_pupil_x = left_iris['x'].mean()
    left_offset = (left_pupil_x - left_center_x) / left_width

    # RIGHT eye
    right_points = frame_data[frame_data['landmark_id'].isin(RIGHT_EYE_CORNERS)]
    right_iris = frame_data[frame_data['landmark_id'].isin(RIGHT_EYE_CENTER_IDS)]

    if len(right_points) < 2 or len(right_iris) < 1:
        continue

    rx0 = right_points.iloc[0]['x']
    rx1 = right_points.iloc[1]['x']
    right_width = abs(rx1 - rx0)
    right_center_x = (rx0 + rx1) / 2
    right_pupil_x = right_iris['x'].mean()
    right_offset = (right_pupil_x - right_center_x) / right_width

    # Average both eyes
    avg_offset = (left_offset + right_offset) / 2

    # Classify gaze
    if avg_offset < -0.1:
        eye_direction = 'left'
    elif avg_offset > 0.1:
        eye_direction = 'right'
    else:
        eye_direction = 'center'

    eye_results.append({
        'frame': frame_num,
        'left_offset': left_offset,
        'right_offset': right_offset,
        'avg_offset': avg_offset,
        'eye_gaze_direction': eye_direction
    })

#  Save eye gaze per frame
eye_df = pd.DataFrame(eye_results)
eye_df.to_csv('/content/eye_gaze_per_frame.csv', index=False)
print(" Saved eye gaze labels per frame to /content/eye_gaze_per_frame.csv")
print(eye_df.head())

# Reload your typing events
import pandas as pd

typing_df = pd.read_csv('/content/typing_with_headpose_alignment.csv')
typing_df['timestamp'] = pd.to_datetime(typing_df['timestamp'])
print(typing_df.head())

import pandas as pd
import numpy as np

# ================================
# 1 LOAD TYPING WITH HEADPOSE DATA
# ================================
typing_df = pd.read_csv('/content/typing_with_headpose_alignment.csv')
print(f" Loaded typing_with_headpose_alignment.csv with {len(typing_df)} rows")
print(" Columns:", typing_df.columns.tolist())

#  Ensure timestamp is proper datetime
print("\n Converting 'timestamp' column to datetime if needed...")
typing_df['timestamp'] = pd.to_datetime(typing_df['timestamp'])
print(" typing_df['timestamp'] type:", typing_df['timestamp'].dtype)

#  Set as index for alignment
typing_df = typing_df.set_index('timestamp').sort_index()
print(" typing_df index:", typing_df.index.dtype)


# ================================
# 2 LOAD EYE GAZE PER FRAME DATA
# ================================
eye_df = pd.read_csv('/content/eye_gaze_per_frame.csv')
print(f"\n Loaded eye_gaze_per_frame.csv with {len(eye_df)} rows")
print(" Columns:", eye_df.columns.tolist())


# ================================
# 3 ADD SYNTHETIC frame_time TO EYE DATA
# ================================
fps = 30
ms_per_frame = 1000 / fps

#  Use video start time from typing_df index
video_start_time = typing_df.index.min()
print(f"\n Using video_start_time = {video_start_time}")

#  Add frame_time to eye_gaze dataframe
eye_df['frame_time'] = pd.to_datetime(video_start_time) + pd.to_timedelta(eye_df['frame'].values * ms_per_frame, unit='ms')
print(" Created 'frame_time' column in eye_df")


# ================================
# 4 ALIGN EYE GAZE DATA TO TYPING EVENTS
# ================================
#  Set frame_time as index
eye_df = eye_df.set_index('frame_time').sort_index()
print(" eye_df index:", eye_df.index.dtype)

#  Nearest join
aligned_eye = eye_df.reindex(
    typing_df.index,
    method='nearest',
    tolerance=pd.Timedelta(milliseconds=100)
)
aligned_eye = aligned_eye.rename(columns={'frame': 'eye_frame'})

print("\n Aligned eye_df to typing_df timestamps")
print(aligned_eye.head())


# ================================
# 5 MERGE INTO TYPING DATA
# ================================
typing_with_eye = typing_df.copy()
typing_with_eye = typing_with_eye.join(aligned_eye[['eye_frame', 'avg_offset', 'eye_gaze_direction']])
typing_with_eye = typing_with_eye.reset_index()
typing_with_eye = typing_with_eye.dropna(subset=['eye_gaze_direction'])

print(f"\n Successfully merged! Resulting rows: {len(typing_with_eye)}")
print(typing_with_eye.head())


# ================================
# 6 SAVE FINAL MERGED CSV
# ================================
output_path = '/content/typing_with_eye_alignment.csv'
typing_with_eye.to_csv(output_path, index=False)
print(f"\n Saved merged typing+eye alignment to {output_path}")

print(typing_df.dtypes)
print(typing_df.head())

typing_df = pd.read_csv('/content/typing_with_headpose_alignment.csv')

# Check what columns you actually have
print(typing_df.columns)

print(df['landmark_id'].max())
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os

#  Load your merged typing+eye alignment data
eye_typing_csv = '/content/typing_with_eye_alignment.csv'
frames_folder = '/content/frames_output'

df = pd.read_csv(eye_typing_csv)
print(f" Loaded {len(df)} typing events with eye alignment!")

#  Check available directions
print(" Distribution of eye_gaze_direction:")
print(df['eye_gaze_direction'].value_counts())

#  Define directions to show
directions = ['left', 'center', 'right']

for direction in directions:
    print(f"\n 5 samples for eye_gaze_direction = {direction}:")
    subset = df[df['eye_gaze_direction'] == direction]

    if subset.empty:
        print(" No data for this category!")
        continue

    subset = subset.head(5)

    for idx, row in subset.iterrows():
        frame_num = int(row['frame'])
        char = row.get('char', '')
        offset = row.get('avg_offset', np.nan)
        timestamp = row['timestamp']

        # Build frame filename
        frame_filename = os.path.join(frames_folder, f"frame_{frame_num:05d}.png")

        # Load the image
        image = cv2.imread(frame_filename)
        if image is None:
            print(f" Could not load frame: {frame_filename}")
            continue

        # Convert to RGB for matplotlib
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Plot
        plt.figure(figsize=(6,6))
        plt.imshow(image_rgb)
        plt.title(
            f"Frame {frame_num} | Char: {char}\n"
            f"Offset: {offset:.3f} | Direction: {direction}\n"
            f"Timestamp: {timestamp}"
        )
        plt.axis('off')
        plt.show()

print(typing_with_eye['avg_offset'].describe())

typing_with_eye['avg_offset'].hist(bins=50)

#  Compute percentiles from your data
left_threshold = typing_with_eye['avg_offset'].quantile(0.25)
right_threshold = typing_with_eye['avg_offset'].quantile(0.75)

print(f" Using dynamic thresholds:")
print(f"Left threshold: {left_threshold:.6f}")
print(f"Right threshold: {right_threshold:.6f}")

#  Apply new labeling
def classify_dynamic(offset):
    if offset < left_threshold:
        return 'left'
    elif offset > right_threshold:
        return 'right'
    else:
        return 'center'

typing_with_eye['eye_gaze_direction'] = typing_with_eye['avg_offset'].apply(classify_dynamic)

#  Check new distribution
print("\n New label distribution:")
print(typing_with_eye['eye_gaze_direction'].value_counts())

#  Save again
typing_with_eye.to_csv('/content/typing_with_eye_alignment_rethresholded.csv', index=False)
print("\n Saved re-labeled data to /content/typing_with_eye_alignment_rethresholded.csv")

import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os

#  Load merged typing+eye CSV with thresholds
typing_with_eye = pd.read_csv('/content/typing_with_eye_alignment_rethresholded.csv')
print(f" Loaded {len(typing_with_eye)} rows with new labels")

#  Load all landmarks for frames
landmarks_df = pd.read_csv('/content/landmarks_all_frames.csv')
print(f" Loaded landmarks: {landmarks_df.shape}")

#  Define frames folder
frames_folder = '/content/frames_output'

#  Define landmark IDs for eyes
LEFT_EYE_IDS = [
    33, 133, 160, 159, 158, 157, 173,
    246, 161, 144, 145, 153, 154, 155
]

RIGHT_EYE_IDS = [
    362, 263, 387, 386, 385, 384, 398,
    466, 388, 373, 374, 380, 381, 382
]

#  Loop over categories
directions = ['left', 'center', 'right']

for direction in directions:
    print(f"\n 5 samples for eye_gaze_direction = {direction}:")
    subset = typing_with_eye[typing_with_eye['eye_gaze_direction'] == direction]

    if subset.empty:
        print(" No data for this category!")
        continue

    subset = subset.head(5)

    for idx, row in subset.iterrows():
        frame_num = int(row['eye_frame']) if 'eye_frame' in row else int(row['frame'])
        char = row.get('char', '')
        offset = row.get('avg_offset', np.nan)
        timestamp = row['timestamp']

        # Build frame filename
        frame_filename = os.path.join(frames_folder, f"frame_{frame_num:05d}.png")

        # Load image
        image = cv2.imread(frame_filename)
        if image is None:
            print(f"❗️ Could not load frame: {frame_filename}")
            continue
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        #  Get landmarks for this frame
        frame_landmarks = landmarks_df[landmarks_df['frame'] == frame_num]

        # Draw landmarks on image
        h, w, _ = image.shape
        plt.figure(figsize=(6, 6))
        plt.imshow(image_rgb)

        # Plot LEFT eye landmarks
        left_eye_pts = frame_landmarks[frame_landmarks['landmark_id'].isin(LEFT_EYE_IDS)]
        plt.scatter(left_eye_pts['x'] * w, left_eye_pts['y'] * h, s=10, c='cyan', label='Left Eye')

        # Plot RIGHT eye landmarks
        right_eye_pts = frame_landmarks[frame_landmarks['landmark_id'].isin(RIGHT_EYE_IDS)]
        plt.scatter(right_eye_pts['x'] * w, right_eye_pts['y'] * h, s=10, c='magenta', label='Right Eye')

        # Title
        plt.title(
            f"Frame {frame_num} | Char: {char}\n"
            f"Offset: {offset:.4f} | Direction: {direction}\n"
            f"Timestamp: {timestamp}"
        )
        plt.axis('off')
        plt.legend()
        plt.show()

import pandas as pd

#  Load your merged file
input_file = '/content/typing_with_eye_alignment_rethresholded.csv'
typing_with_eye = pd.read_csv(input_file)
print(f" Loaded {len(typing_with_eye)} rows from {input_file}")

#  Define classification function
def classify_screen_attention(row):
    pitch = row['pitch']
    yaw = row['yaw']
    eye_offset = row['avg_offset']

    # Head pose thresholds
    if pitch < -10 or pitch > 10:
        return 'not_screen'
    if yaw < -15 or yaw > 15:
        return 'not_screen'

    # Eye offset thresholds (relative to head direction)
    if abs(eye_offset) > 0.03:
        return 'not_screen'

    return 'screen'

#  Apply classification
typing_with_eye['screen_attention'] = typing_with_eye.apply(classify_screen_attention, axis=1)

#  Check result
print("\n Distribution of screen_attention labels:")
print(typing_with_eye['screen_attention'].value_counts())

#  Save new CSV
output_file = '/content/typing_with_screen_attention.csv'
typing_with_eye.to_csv(output_file, index=False)
print(f"\n Saved labeled file to {output_file}")

#  Optional: show a few examples
print("\n Sample rows with new label:")
print(typing_with_eye[['timestamp', 'char', 'pitch', 'yaw', 'avg_offset', 'screen_attention']].head(10))

import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os

#  Load merged typing+eye CSV with thresholds
typing_with_eye = pd.read_csv('/content/typing_with_eye_alignment_rethresholded.csv')
print(f" Loaded {len(typing_with_eye)} rows with new labels")

#  Define frames folder
frames_folder = '/content/frames_output'

#  Loop over categories
directions = ['left', 'center', 'right']

for direction in directions:
    print(f"\n 5 samples for eye_gaze_direction = {direction}:")
    subset = typing_with_eye[typing_with_eye['eye_gaze_direction'] == direction]

    if subset.empty:
        print(" No data for this category!")
        continue

    subset = subset.head(5)

    for idx, row in subset.iterrows():
        frame_num = int(row['eye_frame']) if 'eye_frame' in row else int(row['frame'])
        char = row.get('char', '')
        offset = row.get('avg_offset', np.nan)
        timestamp = row['timestamp']

        # Build frame filename
        frame_filename = os.path.join(frames_folder, f"frame_{frame_num:05d}.png")

        # Load image
        image = cv2.imread(frame_filename)
        if image is None:
            print(f" Could not load frame: {frame_filename}")
            continue

        # Convert BGR to RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Plot the plain image with title
        plt.figure(figsize=(6, 6))
        plt.imshow(image_rgb)
        plt.title(
            f"Frame {frame_num} | Char: {char}\n"
            f"Offset: {offset:.4f} | Direction: {direction}\n"
            f"Timestamp: {timestamp}"
        )
        plt.axis('off')
        plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os
import numpy as np

#  Load merged typing+eye CSV with thresholds
typing_with_eye = pd.read_csv('/content/typing_with_eye_alignment_rethresholded.csv')
print(f" Loaded {len(typing_with_eye)} rows with new labels")

#  Load all landmarks for frames
landmarks_df = pd.read_csv('/content/landmarks_all_frames.csv')
print(f" Loaded landmarks: {landmarks_df.shape}")

#  Define frames folder
frames_folder = '/content/frames_output'

#  Define nose landmark ID (1 in MediaPipe)
NOSE_TIP_ID = 1

#  Adjustable scale for eye offset
EYE_OFFSET_SCALE = 300  # You can tweak this!

#  Arrow length for visualization
ARROW_LENGTH = 300

#  Define function to draw 2D arrow with yaw + pitch
def draw_gaze_arrow_on_image(image_rgb, nose_point, yaw_deg, pitch_deg, color='red', length=100):
    yaw_rad = np.radians(yaw_deg)
    pitch_rad = np.radians(pitch_deg)

    # Compute x and y deltas
    dx = length * np.sin(yaw_rad)
    dy = length * np.sin(-pitch_rad)  # Negative because image y-axis is top-down

    x0, y0 = nose_point
    x1 = x0 + dx
    y1 = y0 + dy

    plt.imshow(image_rgb)
    plt.arrow(x0, y0, dx, dy, color=color, head_width=10, length_includes_head=True)

#  Loop over categories
directions = ['left', 'center', 'right']

for direction in directions:
    print(f"\n 5 samples for eye_gaze_direction = {direction}:")
    subset = typing_with_eye[typing_with_eye['eye_gaze_direction'] == direction]

    if subset.empty:
        print(" No data for this category!")
        continue

    subset = subset.head(10)

    for idx, row in subset.iterrows():
        frame_num = int(row['eye_frame']) if 'eye_frame' in row else int(row['frame'])
        char = row.get('char', '')
        offset = row.get('avg_offset', np.nan)
        timestamp = row['timestamp']

        # Build frame filename
        frame_filename = os.path.join(frames_folder, f"frame_{frame_num:05d}.png")

        # Load image
        image = cv2.imread(frame_filename)
        if image is None:
            print(f" Could not load frame: {frame_filename}")
            continue
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w, _ = image.shape

        #  Get nose landmark for this frame
        frame_landmarks = landmarks_df[landmarks_df['frame'] == frame_num]
        nose_landmark = frame_landmarks[frame_landmarks['landmark_id'] == NOSE_TIP_ID]
        if nose_landmark.empty:
            print(f" No nose landmark found for frame {frame_num}")
            continue
        nose_x = nose_landmark['x'].values[0] * w
        nose_y = nose_landmark['y'].values[0] * h

        #  Compute combined gaze angles (head + eye)
        combined_yaw = row['yaw'] - (offset * EYE_OFFSET_SCALE)
        combined_pitch = row['pitch']

        #  Flip to user-centric perspective
        final_gaze_yaw = -combined_yaw

        #  Classify
        if final_gaze_yaw < -5:
            combined_label = 'right'
        elif final_gaze_yaw > 5:
            combined_label = 'left'
        else:
            combined_label = 'center'

        #  Plot with arrow
        plt.figure(figsize=(6, 6))
        draw_gaze_arrow_on_image(image_rgb, (nose_x, nose_y), final_gaze_yaw, combined_pitch, color='red', length=ARROW_LENGTH)
        plt.title(
            f"Frame {frame_num} | Char: {char}\n"
            f"Gaze Yaw: {final_gaze_yaw:.1f}° | Pitch: {combined_pitch:.1f}° | Combined Direction: {combined_label}\n"
            f"Timestamp: {timestamp}"
        )
        plt.axis('off')
        plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os
import numpy as np

#  Load CSV with head + eye alignment
typing_with_eye = pd.read_csv('/content/typing_with_eye_alignment_rethresholded.csv')
print(f" Loaded {len(typing_with_eye)} rows")

#  Parameters
EYE_OFFSET_SCALE = 300
THRESHOLD_DEG = 5

#  Compute combined_yaw and final user-centric gaze
def compute_final_gaze_yaw(row, scale=300):
    combined_yaw = row['yaw'] - (row['avg_offset'] * scale)
    return -combined_yaw

typing_with_eye['final_gaze_yaw'] = typing_with_eye.apply(compute_final_gaze_yaw, axis=1)

#  Classify direction
def classify_direction(yaw, threshold=5):
    if yaw < -threshold:
        return 'left'
    elif yaw > threshold:
        return 'right'
    else:
        return 'center'

typing_with_eye['combined_category'] = typing_with_eye['final_gaze_yaw'].apply(lambda x: classify_direction(x, THRESHOLD_DEG))

#  Save to new CSV
typing_with_eye.to_csv('/content/typing_with_combined_category.csv', index=False)
print("\n Saved new CSV with combined_category column!")

#  Show distribution
print("\n Distribution of combined_category:")
print(typing_with_eye['combined_category'].value_counts())

#  Load landmarks for nose
landmarks_df = pd.read_csv('/content/landmarks_all_frames.csv')
frames_folder = '/content/frames_output'
NOSE_TIP_ID = 1

#  Arrow drawing function
def draw_gaze_arrow_on_image(image_rgb, nose_point, yaw_deg, pitch_deg, color='red', length=100):
    yaw_rad = np.radians(yaw_deg)
    pitch_rad = np.radians(pitch_deg)
    dx = length * np.sin(yaw_rad)
    dy = length * np.sin(-pitch_rad)
    x0, y0 = nose_point
    plt.imshow(image_rgb)
    plt.arrow(x0, y0, dx, dy, color=color, head_width=10, length_includes_head=True)

#  Loop over combined categories and show samples
for category in ['left', 'center', 'right']:
    print(f"\n 5 samples for combined_category = {category}:")
    subset = typing_with_eye[typing_with_eye['combined_category'] == category]

    if subset.empty:
        print("❗️ No data for this category!")
        continue

    # Print 5 rows
    print(subset[['frame', 'timestamp', 'char', 'final_gaze_yaw', 'pitch', 'combined_category']].head(5))

    # Show 5 images
    for idx, row in subset.head(5).iterrows():
        frame_num = int(row['eye_frame']) if 'eye_frame' in row else int(row['frame'])
        char = row.get('char', '')
        timestamp = row['timestamp']
        final_gaze_yaw = row['final_gaze_yaw']
        pitch = row['pitch']
        combined_label = row['combined_category']

        # Load image
        frame_filename = os.path.join(frames_folder, f"frame_{frame_num:05d}.png")
        image = cv2.imread(frame_filename)
        if image is None:
            print(f"❗️ Could not load frame: {frame_filename}")
            continue
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w, _ = image.shape

        # Get nose landmark
        frame_landmarks = landmarks_df[landmarks_df['frame'] == frame_num]
        nose_landmark = frame_landmarks[frame_landmarks['landmark_id'] == NOSE_TIP_ID]
        if nose_landmark.empty:
            print(f" No nose landmark found for frame {frame_num}")
            continue
        nose_x = nose_landmark['x'].values[0] * w
        nose_y = nose_landmark['y'].values[0] * h

        # Plot with arrow
        plt.figure(figsize=(6, 6))
        draw_gaze_arrow_on_image(image_rgb, (nose_x, nose_y), final_gaze_yaw, pitch, color='red', length=100)
        plt.title(
            f"Frame {frame_num} | Char: {char}\n"
            f"Combined Direction: {combined_label}\n"
            f"Gaze Yaw: {final_gaze_yaw:.1f}° | Pitch: {pitch:.1f}°\n"
            f"Timestamp: {timestamp}"
        )
        plt.axis('off')
        plt.show()

import pandas as pd

#  Load the labeled CSV
df = pd.read_csv('/content/typing_with_combined_category.csv', parse_dates=['timestamp'])
print(f" Loaded {len(df)} rows")

#  Check unique categories
print("\n Unique categories in data:")
print(df['combined_category'].unique())

#  Filter Center and Right
center_df = df[df['combined_category'] == 'center']
right_df = df[df['combined_category'] == 'right']

print("\n Counts:")
print(f"Center rows: {len(center_df)}")
print(f"Right rows: {len(right_df)}")

#  Print typed characters
print("\n Characters typed while Center (not looking at screen):")
print(center_df[['timestamp', 'char']])

print("\n Characters typed while Right (looking at screen):")
print(right_df[['timestamp', 'char']])

#  Analyze contiguous center periods
center_df = center_df.sort_values('timestamp').reset_index(drop=True)
center_df['block_id'] = (center_df['timestamp'].diff() > pd.Timedelta(seconds=1)).cumsum()

print("\n Detected Center periods:")
periods = []
for block_id, group in center_df.groupby('block_id'):
    start_time = group['timestamp'].min()
    end_time = group['timestamp'].max()
    duration = (end_time - start_time).total_seconds()
    num_chars = group['char'].notna().sum()
    chars_typed = group['char'].dropna().tolist()

    periods.append({
        'Block ID': block_id,
        'Start Time': start_time,
        'End Time': end_time,
        'Duration (seconds)': duration,
        'Chars Typed Count': num_chars,
        'Chars Typed': chars_typed
    })

periods_df = pd.DataFrame(periods)
print(periods_df)

#  Optional: Save analysis to CSV
periods_df.to_csv('/content/center_gaze_periods_analysis.csv', index=False)
print("\n Saved center periods analysis to /content/center_gaze_periods_analysis.csv")

import pandas as pd
import matplotlib.pyplot as plt

#  Load CSV
df = pd.read_csv('/content/typing_with_combined_category.csv')
print(f" Loaded {len(df)} rows")

#  Clean: drop NaN or empty chars
df = df.dropna(subset=['char'])
df = df[df['char'].str.strip() != '']
print(f" Filtered to {len(df)} rows with valid chars")

#  Split by gaze category
center_chars = df[df['combined_category'] == 'center']['char']
right_chars = df[df['combined_category'] == 'right']['char']

#  Count frequencies
center_counts = center_chars.value_counts().sort_index()
right_counts = right_chars.value_counts().sort_index()

#  Merge into one DataFrame
all_chars = sorted(set(center_counts.index) | set(right_counts.index))
data = pd.DataFrame({
    'Character': all_chars,
    'Center': [center_counts.get(c, 0) for c in all_chars],
    'Right': [right_counts.get(c, 0) for c in all_chars]
})

print("\n Counts DataFrame:")
print(data)

#  Plot
plt.figure(figsize=(16, 6))
bar_width = 0.4
positions = range(len(data))

plt.bar([p - bar_width/2 for p in positions], data['Center'], width=bar_width, label='Center (Not Looking at Screen)')
plt.bar([p + bar_width/2 for p in positions], data['Right'], width=bar_width, label='Right (Looking at Screen)')

plt.xticks(positions, data['Character'], rotation=90)
plt.xlabel('Character')
plt.ylabel('Count')
plt.title('Character Counts: Center vs Right Gaze')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

import pandas as pd

#  Load data
df = pd.read_csv('/content/typing_with_combined_category.csv', parse_dates=['timestamp'])
print(f" Loaded {len(df)} rows")

#  Clean: remove blank / NaN chars
df = df.dropna(subset=['char'])
df = df[df['char'].str.strip() != '']
print(f" Filtered to {len(df)} valid rows with chars")

#  Filter CENTER (user looking at keyboard)
center_df = df[df['combined_category'] == 'center'].sort_values('timestamp').reset_index(drop=True)
print(f" Center rows: {len(center_df)}")

#  Identify contiguous center blocks
center_df['block_id'] = (center_df['timestamp'].diff() > pd.Timedelta(seconds=1)).cumsum()

#  Analyze blocks
char_durations = []

for block_id, group in center_df.groupby('block_id'):
    if len(group) < 2:
        continue  # skip too-short single events

    start_time = group['timestamp'].min()
    end_time = group['timestamp'].max()
    block_duration = (end_time - start_time).total_seconds()

    if block_duration <= 0:
        continue

    # Distribute duration equally to each char typed in this block
    num_chars = group['char'].notna().sum()
    if num_chars == 0:
        continue

    duration_per_char = block_duration / num_chars

    for c in group['char'].dropna():
        char_durations.append({
            'char': c,
            'duration': duration_per_char
        })

# Create DataFrame of per-char durations
dur_df = pd.DataFrame(char_durations)
print(f" Created durations DataFrame: {dur_df.shape}")

#  Aggregate
summary = dur_df.groupby('char').agg(
    Total_Center_Looking_Time=('duration', 'sum'),
    Count_Center_Looking=('duration', 'count')
).reset_index()

summary['Avg_Looking_Time_Per_Event'] = summary['Total_Center_Looking_Time'] / summary['Count_Center_Looking']

#  Rank hardest
summary = summary.sort_values('Avg_Looking_Time_Per_Event', ascending=False).reset_index(drop=True)

#  Show top hardest letters
print("\n Top Characters Requiring More Keyboard-Looking (Center):")
print(summary.head(10))

#  Optional: Save
summary.to_csv('/content/hardest_characters_keyboard_center.csv', index=False)
print("\n Saved analysis to /content/hardest_characters_keyboard_center.csv")

import pandas as pd

#  Load the result CSV
summary = pd.read_csv('/content/hardest_characters_keyboard_center.csv')

#  Sort in DESCENDING order of average looking time
summary_sorted = summary.sort_values('Avg_Looking_Time_Per_Event', ascending=False).reset_index(drop=True)

#  Select Top 10
top10 = summary_sorted.head(10)

#  Print it nicely
print("\n TOP 10 HARDEST CHARACTERS (Descending by Average Looking Time):")
print(top10.to_string(index=False))

import pandas as pd
import matplotlib.pyplot as plt

#  Load data
df = pd.read_csv('/content/typing_with_combined_category.csv', parse_dates=['timestamp'])
print(f" Loaded {len(df)} rows")

#  Sort by time
df = df.sort_values('timestamp').reset_index(drop=True)

#  Function to compute total looking time for a category
def compute_total_looking_time(category_name):
    category_df = df[df['combined_category'] == category_name]
    if category_df.empty:
        return 0.0

    category_df = category_df.sort_values('timestamp').reset_index(drop=True)
    category_df['block_id'] = (category_df['timestamp'].diff() > pd.Timedelta(seconds=1)).cumsum()

    total_time = 0.0
    for _, group in category_df.groupby('block_id'):
        if len(group) < 2:
            continue
        duration = (group['timestamp'].max() - group['timestamp'].min()).total_seconds()
        if duration > 0:
            total_time += duration
    return total_time

#  Compute times
keyboard_time = compute_total_looking_time('center')
screen_time = compute_total_looking_time('right')
total_typing_time = keyboard_time + screen_time

print("\n TOTAL TIME (seconds):")
print(f"Keyboard-looking (center): {keyboard_time:.2f} seconds")
print(f"Screen-looking (right): {screen_time:.2f} seconds")
print(f"Total typing session time (sum): {total_typing_time:.2f} seconds")

#  Pie chart
labels = ['Keyboard-looking (center)', 'Screen-looking (right)']
sizes = [keyboard_time, screen_time]
colors = ['lightcoral', 'lightskyblue']

plt.figure(figsize=(6, 6))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=140)
plt.title('Proportion of Time Spent Looking at Keyboard vs Screen')
plt.axis('equal')
plt.show()

